{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score,confusion_matrix, accuracy_score, classification_report, precision_recall_fscore_support, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold,KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('E:\\Data Mining\\S2\\Projek Akhir\\dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CID</th>\n",
       "      <th>Accession</th>\n",
       "      <th>class</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11671467</td>\n",
       "      <td>Q9UHD2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.038409</td>\n",
       "      <td>0.048011</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.009602</td>\n",
       "      <td>0.038409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57469</td>\n",
       "      <td>Q9UHD2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.038409</td>\n",
       "      <td>0.048011</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.009602</td>\n",
       "      <td>0.038409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46831374</td>\n",
       "      <td>Q9UHD2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.038409</td>\n",
       "      <td>0.048011</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.009602</td>\n",
       "      <td>0.038409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44592367</td>\n",
       "      <td>Q9UHD2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.038409</td>\n",
       "      <td>0.048011</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.009602</td>\n",
       "      <td>0.038409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46831378</td>\n",
       "      <td>Q9UHD2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.038409</td>\n",
       "      <td>0.048011</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.009602</td>\n",
       "      <td>0.038409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 904 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CID Accession  class  1  2  3  4  5  6  7  ...         M         N  \\\n",
       "0  11671467    Q9UHD2      1  1  1  1  0  0  0  0  ...  0.030178  0.041152   \n",
       "1     57469    Q9UHD2      0  1  1  1  0  0  0  0  ...  0.030178  0.041152   \n",
       "2  46831374    Q9UHD2      0  1  1  1  0  0  0  0  ...  0.030178  0.041152   \n",
       "3  44592367    Q9UHD2      0  1  1  1  0  0  0  0  ...  0.030178  0.041152   \n",
       "4  46831378    Q9UHD2      0  1  1  1  0  0  0  0  ...  0.030178  0.041152   \n",
       "\n",
       "          P         Q         R         S         T         V         W  \\\n",
       "0  0.032922  0.038409  0.048011  0.057613  0.061728  0.061728  0.009602   \n",
       "1  0.032922  0.038409  0.048011  0.057613  0.061728  0.061728  0.009602   \n",
       "2  0.032922  0.038409  0.048011  0.057613  0.061728  0.061728  0.009602   \n",
       "3  0.032922  0.038409  0.048011  0.057613  0.061728  0.061728  0.009602   \n",
       "4  0.032922  0.038409  0.048011  0.057613  0.061728  0.061728  0.009602   \n",
       "\n",
       "          Y  \n",
       "0  0.038409  \n",
       "1  0.038409  \n",
       "2  0.038409  \n",
       "3  0.038409  \n",
       "4  0.038409  \n",
       "\n",
       "[5 rows x 904 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input_matrix=dataset.drop(['CID','Accession','class'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_label_vector=dataset['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(my_input_matrix,my_label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0001509630481910932\n",
      "2 0.00035520522936084394\n",
      "3 0.0026577079312961305\n",
      "4 0.0026175145944521434\n",
      "7 3.1276701962414245e-05\n",
      "10 8.102244513150354e-05\n",
      "11 9.996488722601555e-05\n",
      "12 0.00032401149087055934\n",
      "13 0.0018846118509535362\n",
      "14 0.0022513383022057143\n",
      "15 0.0005058812417116154\n",
      "16 0.002377636363058823\n",
      "17 0.0028032801811883534\n",
      "18 0.002503949476832492\n",
      "19 0.0008670741200308543\n",
      "20 0.0021080528669284522\n",
      "21 0.002772798101572557\n",
      "22 0.0026640050651014404\n",
      "23 0.00011723331641006202\n",
      "24 0.0015474027084831892\n",
      "25 0.001100414211162187\n",
      "26 0.00047225810261884997\n",
      "27 0.0007746469871046159\n",
      "28 2.182534346056067e-05\n",
      "29 1.6944293749084944e-06\n",
      "31 0.000565446958801996\n",
      "32 0.00011965507520204749\n",
      "33 5.862866123989285e-05\n",
      "34 0.0014895536094753203\n",
      "35 0.0005195704674822369\n",
      "36 3.006885821477817e-05\n",
      "38 0.0009080610209832433\n",
      "39 0.0010832754636133108\n",
      "40 0.0001092015325465955\n",
      "44 0.0004180392601405778\n",
      "45 0.00011021908709602868\n",
      "46 1.587473648528128e-06\n",
      "47 0.00010060269100187411\n",
      "48 6.074035580854828e-05\n",
      "49 4.270675460424844e-05\n",
      "62 0.00014047954378599894\n",
      "63 2.4549503635096597e-05\n",
      "66 0.00029308219594183124\n",
      "67 3.1958702578031345e-05\n",
      "83 7.378932779560831e-06\n",
      "94 4.553078742806476e-05\n",
      "95 7.334488735836784e-05\n",
      "114 2.4090891406104033e-05\n",
      "116 0.000524645059479725\n",
      "117 0.00038815980356728596\n",
      "118 2.4148643562366497e-05\n",
      "119 0.00010044716717770772\n",
      "123 5.9904917742342104e-05\n",
      "124 9.709296175171424e-05\n",
      "130 0.00037865693796890405\n",
      "131 0.000124551796719601\n",
      "132 0.00022562398069590498\n",
      "133 0.00031189616404786637\n",
      "137 5.0823920073392845e-05\n",
      "138 1.3699145679852712e-05\n",
      "140 6.95873971276955e-05\n",
      "144 0.0023372189145869217\n",
      "145 0.0006796496746242898\n",
      "146 0.002505795544053212\n",
      "147 0.002276744038449791\n",
      "151 0.0016833914865024236\n",
      "152 8.285582516947929e-05\n",
      "153 0.001422332104231279\n",
      "154 0.001610834153919039\n",
      "158 0.0002324416462351103\n",
      "160 0.00027968653745837736\n",
      "161 0.00037348698036289635\n",
      "165 0.0001740606148750009\n",
      "167 0.00010508036614319881\n",
      "168 7.633018417485744e-05\n",
      "172 3.1160646070290514e-05\n",
      "174 3.71385145715715e-05\n",
      "175 2.7895152676933755e-05\n",
      "179 0.000525481410455231\n",
      "180 0.0008530147379831972\n",
      "181 0.0019652605050911125\n",
      "182 0.002285535987418043\n",
      "186 0.0015364129909113489\n",
      "187 0.0020258447532699488\n",
      "188 0.0018107003858750545\n",
      "189 0.0021567042679649737\n",
      "193 0.0021066945193617057\n",
      "194 0.001586032621947819\n",
      "195 0.0005838552088766971\n",
      "196 0.0004913693799969864\n",
      "200 0.0019509707856804504\n",
      "201 0.0005711124313590766\n",
      "202 0.0001505217730133784\n",
      "203 0.00013646380565419033\n",
      "207 0.0008965226365883401\n",
      "208 7.717923331655057e-05\n",
      "209 2.200037946551374e-05\n",
      "210 1.0265407746732707e-05\n",
      "214 0.0009974092567992827\n",
      "215 0.00011046670524368713\n",
      "216 0.0007179221110886124\n",
      "217 0.0006156528950392765\n",
      "221 6.059720661245857e-05\n",
      "224 4.6194158396721194e-05\n",
      "228 0.00047000621639851215\n",
      "229 2.3534161220661413e-05\n",
      "230 0.00026626504164291173\n",
      "231 0.0003379829722780174\n",
      "235 6.3439317759863446e-06\n",
      "242 0.0020900532510228064\n",
      "243 0.00046852104331600026\n",
      "244 0.002037631522419218\n",
      "245 0.0015592398586381168\n",
      "249 0.002566178102670833\n",
      "250 0.0009033281165915999\n",
      "251 0.0029944013704528106\n",
      "252 0.0022838988169303727\n",
      "256 0.00033499465509098365\n",
      "257 0.0015568900139276377\n",
      "258 0.0013254134438754654\n",
      "259 0.0037587938836994178\n",
      "260 0.0023814584555454975\n",
      "261 0.0031562280948984845\n",
      "262 0.0029390990754269066\n",
      "263 0.0019474046212532709\n",
      "275 3.393910031003935e-05\n",
      "276 1.0958486694779861e-05\n",
      "277 2.6837599757388804e-05\n",
      "278 1.8252556494232525e-05\n",
      "284 5.8052280429725095e-05\n",
      "285 5.9959851010162004e-05\n",
      "286 0.0004776457839965351\n",
      "287 0.0007415680526584298\n",
      "288 0.0013606730650978507\n",
      "293 0.0003754642730912648\n",
      "294 0.0017148960125272568\n",
      "295 0.0008091331324792118\n",
      "297 3.581872451789008e-05\n",
      "298 0.0004266437414737409\n",
      "299 9.282437078570842e-05\n",
      "300 0.003499563075205486\n",
      "301 0.0018975648643562695\n",
      "302 0.0008596066649753925\n",
      "305 3.716616730805756e-05\n",
      "306 0.000583880773213784\n",
      "309 0.0034132254372809777\n",
      "310 3.952363877582729e-05\n",
      "315 0.0005135623942722267\n",
      "317 5.693698341408858e-05\n",
      "328 0.0003645262927712073\n",
      "329 0.00045686548861120926\n",
      "330 1.5351784526802388e-05\n",
      "331 0.00048439487609186954\n",
      "332 4.942596681637546e-05\n",
      "333 0.00010669012285341651\n",
      "334 0.0005531768682432993\n",
      "335 0.0018449544091035533\n",
      "336 0.0031175664100252065\n",
      "337 0.0012108039345410666\n",
      "338 0.0015267812244228495\n",
      "339 0.0038867925738800624\n",
      "340 0.002554819063090708\n",
      "341 0.001081328010440953\n",
      "342 0.0018756239372781447\n",
      "343 0.0007747112278698723\n",
      "344 8.672433689640042e-05\n",
      "345 0.0001206113585384381\n",
      "346 0.002635366082908577\n",
      "347 0.002378891114385282\n",
      "348 0.00013627198893230953\n",
      "349 0.00041654320772658014\n",
      "350 0.0009536235231482205\n",
      "351 8.480986509379828e-05\n",
      "352 0.0003688953636074367\n",
      "353 0.0008282971128306848\n",
      "354 0.0010354596735587017\n",
      "355 9.926507083053892e-07\n",
      "356 0.00042983872196939056\n",
      "357 0.0007016766842423361\n",
      "358 0.001658995013963737\n",
      "359 0.001206459907111328\n",
      "360 0.001465843683339459\n",
      "361 1.6769961931795325e-05\n",
      "362 6.941201357850286e-05\n",
      "363 0.0010190827356621123\n",
      "364 0.0008869510400068979\n",
      "365 0.001148427031374378\n",
      "366 0.0024103893662199573\n",
      "367 0.0028013709789552286\n",
      "368 0.0002449306935181384\n",
      "369 0.0011580828205524348\n",
      "370 6.718363156198764e-06\n",
      "371 0.00040142596120585456\n",
      "372 0.0005381824468922375\n",
      "373 0.0021000671857620626\n",
      "374 0.001637187345530494\n",
      "375 0.0019903438460368945\n",
      "376 0.0028665787741086364\n",
      "377 0.0012114506390208675\n",
      "378 0.0018934937918066498\n",
      "379 0.0014709844065725719\n",
      "380 0.0013323477842246394\n",
      "381 0.0023340146362309873\n",
      "382 0.0007819846174947329\n",
      "383 0.0007659584714232462\n",
      "384 0.0008998897530734741\n",
      "385 0.0006835268157541283\n",
      "386 0.0025853536049671114\n",
      "387 0.0017619601278677142\n",
      "388 0.0011059217816029698\n",
      "389 0.0009926027462783282\n",
      "390 0.001297862345998239\n",
      "391 0.0006623400363394373\n",
      "392 0.004741106318910545\n",
      "393 0.003497788370358087\n",
      "394 0.0033786515930069594\n",
      "395 0.001032196543393623\n",
      "396 0.0007724940935704312\n",
      "397 0.0009904425005135594\n",
      "398 0.0010382824578722624\n",
      "399 0.001591870754915238\n",
      "400 0.003977584701430923\n",
      "401 0.0010502544375454751\n",
      "402 0.0003408730252257778\n",
      "403 0.00019830192803015596\n",
      "404 0.0013502254580296923\n",
      "405 8.842085310874628e-05\n",
      "406 0.0018630708299711633\n",
      "407 0.0041443899243675895\n",
      "408 0.0003342027327792881\n",
      "409 4.388466335795473e-05\n",
      "410 0.00022276496024490561\n",
      "411 0.000306626722443305\n",
      "412 0.0005848017778553084\n",
      "413 0.001245526283366326\n",
      "414 5.656140198020298e-05\n",
      "415 0.0010569266918333694\n",
      "416 5.0114095173308524e-06\n",
      "417 0.00025034009303294766\n",
      "418 0.0003705586381029502\n",
      "419 0.0023516218550981606\n",
      "420 0.0065870491056378155\n",
      "421 0.0015173744457042295\n",
      "422 0.0008548675509468874\n",
      "423 0.0013759285225418147\n",
      "424 0.0006015804169757067\n",
      "426 0.0005097679610261832\n",
      "428 0.0002352968543924935\n",
      "429 3.294355599837041e-05\n",
      "430 0.00523135135189219\n",
      "431 0.0009192310339950446\n",
      "432 0.0014699871272039903\n",
      "433 0.002048422155333062\n",
      "435 0.0003400485245425292\n",
      "436 0.0016648099848269474\n",
      "437 0.0004945594160555322\n",
      "438 0.001393194675972991\n",
      "439 0.0014115447220669664\n",
      "440 0.0038028791355857955\n",
      "441 0.003284169497594322\n",
      "442 0.00026583846110778413\n",
      "443 0.0013007131489685827\n",
      "444 0.001542771761661782\n",
      "446 0.0019456932319270085\n",
      "447 0.00036324665654761305\n",
      "448 0.0018987584094986687\n",
      "449 0.0006325427003109706\n",
      "450 0.0009214621454673999\n",
      "451 0.002094786878608838\n",
      "452 0.004223878760910852\n",
      "453 0.0025865399291046713\n",
      "454 0.0021596464761270413\n",
      "455 0.0006665206430006836\n",
      "456 0.0003734783107329602\n",
      "457 0.0006139904891706047\n",
      "458 0.0009563831924997049\n",
      "459 0.0011327236139123088\n",
      "460 0.0008035273389587881\n",
      "461 0.0002317887479035647\n",
      "462 0.0011350283624122178\n",
      "463 0.0012562098474959414\n",
      "465 0.001093142300228926\n",
      "466 0.0010194438834762829\n",
      "467 0.0013970096644023483\n",
      "468 0.001172598063140792\n",
      "469 0.00023092355428352214\n",
      "471 0.0005387127415394442\n",
      "472 0.0002842308609825785\n",
      "473 0.0012364440200117526\n",
      "474 0.0003312553345521658\n",
      "475 0.00022958530586179033\n",
      "476 0.0002777696161760312\n",
      "477 0.0010054848848333041\n",
      "478 0.000889598747909054\n",
      "479 8.393646943080673e-05\n",
      "480 8.156543098766068e-05\n",
      "481 0.0004704355706941459\n",
      "482 2.329150036281882e-05\n",
      "483 0.0014259682327570106\n",
      "484 0.0005266164733925395\n",
      "485 0.001341608049771358\n",
      "486 0.0010800093494397654\n",
      "487 0.0006635843513377727\n",
      "488 0.0015728478235037824\n",
      "489 0.001173897034342329\n",
      "490 0.0007457496829582866\n",
      "491 0.0002938258967134966\n",
      "492 0.0014131547102042385\n",
      "493 0.00016264980384971616\n",
      "494 0.0018345240433571145\n",
      "495 0.00047808714417960763\n",
      "496 0.0013856589246678465\n",
      "497 0.0006438492588565979\n",
      "498 0.0009827880633986388\n",
      "499 0.0011738523793140953\n",
      "500 0.001121396487916628\n",
      "501 0.0007883474109117898\n",
      "502 0.0008821422162802748\n",
      "503 0.001388408367467663\n",
      "504 0.0006331044831932722\n",
      "505 0.001224053922165593\n",
      "506 3.213775693847042e-05\n",
      "507 0.0012278004736497941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508 0.0010078895121735506\n",
      "509 0.0006047354084342346\n",
      "510 0.0003034487227826654\n",
      "511 8.778274392485762e-05\n",
      "512 0.0008404440810373758\n",
      "514 0.0003508951765411765\n",
      "515 0.0007816802037169258\n",
      "516 0.0014388545201363518\n",
      "517 0.0008015924712870769\n",
      "518 0.0008168959327996764\n",
      "519 0.000497717207205025\n",
      "520 0.0011879379423221108\n",
      "521 0.0005326669632990538\n",
      "522 0.0012231174789635525\n",
      "523 0.000949553742968261\n",
      "524 0.0016767799611003675\n",
      "525 0.0003860616609211468\n",
      "527 9.741237749443497e-05\n",
      "528 0.0021067262063955896\n",
      "529 0.007178119747472336\n",
      "530 9.061482993691552e-05\n",
      "531 0.0012844770110850318\n",
      "532 0.0007656919588707428\n",
      "533 0.000913721367400359\n",
      "534 0.0003545663091582316\n",
      "535 0.00022406750266856166\n",
      "536 0.0019350501186915494\n",
      "537 0.0039387154380626605\n",
      "538 0.0012181913027131082\n",
      "539 0.0017933303958307678\n",
      "540 0.001321368665150862\n",
      "541 0.0025892832401253267\n",
      "542 0.0008857069128340983\n",
      "543 0.0007560917482964866\n",
      "544 0.0009469701554774669\n",
      "545 0.0005230777458165466\n",
      "546 0.0012011557942159293\n",
      "547 0.0012807755193046117\n",
      "548 0.001514653637986281\n",
      "549 0.0012119976189870283\n",
      "550 0.0012608097326861404\n",
      "551 0.0007718972368536284\n",
      "552 4.739424331474477e-05\n",
      "553 0.0003467399891263496\n",
      "554 0.0025123982954132927\n",
      "555 0.00025705817897030624\n",
      "556 0.0016160709260772278\n",
      "557 0.000404336251984406\n",
      "558 1.574263962104373e-05\n",
      "559 0.0009526345226597391\n",
      "560 0.0021608876675865444\n",
      "561 0.001838513216418744\n",
      "562 0.00013416980657747946\n",
      "563 3.5115773191593713e-06\n",
      "564 0.00025511862918946033\n",
      "565 0.0004132325640590396\n",
      "566 0.001039768491247231\n",
      "567 0.001847469637359014\n",
      "568 0.0017708026617810316\n",
      "569 0.001597231904274883\n",
      "570 0.002691386602907107\n",
      "571 0.0006141164768536649\n",
      "572 0.0035323194403210855\n",
      "573 0.0011861603945470805\n",
      "574 0.0014405409228414353\n",
      "575 0.0009430531171235279\n",
      "576 0.00039856958506921615\n",
      "577 0.0016800570131903416\n",
      "578 0.00256882807975158\n",
      "579 0.00045728971828210485\n",
      "580 0.0016992833510472737\n",
      "581 0.0017954396147949667\n",
      "582 0.0010299461505806611\n",
      "583 0.00024885822125484807\n",
      "584 3.2145903072382945e-05\n",
      "585 0.00040301248035474205\n",
      "586 0.0014877571615656913\n",
      "587 0.0010197639069227384\n",
      "588 3.443486171609738e-05\n",
      "589 0.0007519820419739623\n",
      "590 0.0007917756318953561\n",
      "591 0.0006905567684503746\n",
      "592 0.0009057584131128887\n",
      "593 0.0009160694728644421\n",
      "594 0.002630845799442107\n",
      "595 0.001604387415483631\n",
      "596 0.0005515324384763628\n",
      "597 0.0011579106674504311\n",
      "598 0.0018489508719055964\n",
      "599 0.0007865860083136396\n",
      "600 0.0004929765916529142\n",
      "601 0.001928617510370193\n",
      "602 0.0017357102872389673\n",
      "603 0.006742479295062644\n",
      "604 0.00041873486058150285\n",
      "605 0.0012410128906550043\n",
      "606 0.0007098251572252015\n",
      "607 0.0009827722975665294\n",
      "608 0.0013246170078354653\n",
      "609 0.000382548526801469\n",
      "610 0.00019063005790533546\n",
      "611 0.0002996210510794188\n",
      "612 0.00301562777928355\n",
      "613 0.002155375741440779\n",
      "614 0.0011445209447512586\n",
      "615 0.002116735219551042\n",
      "616 0.0015516998370315735\n",
      "617 0.000892094677066108\n",
      "618 0.002696177967023745\n",
      "619 0.0005653027368317766\n",
      "620 0.0010400904496380501\n",
      "621 0.0015925174071948034\n",
      "622 0.001654883809652342\n",
      "623 0.0001973440923161339\n",
      "624 0.0018087835643103628\n",
      "625 0.0010085194276023698\n",
      "626 0.0006499007295194511\n",
      "627 0.0008698036859667663\n",
      "629 0.0015626716152458864\n",
      "630 0.00024936466919457006\n",
      "631 0.0005528854195163222\n",
      "632 0.00032140153699096354\n",
      "633 0.0013875375492636746\n",
      "634 0.0014197875361342707\n",
      "635 0.0004474643789341966\n",
      "636 0.0008521851426904027\n",
      "637 0.0019251053312899242\n",
      "638 0.0012692343106860966\n",
      "639 0.001885343111788769\n",
      "640 0.0011781754390968872\n",
      "641 0.0003480702760407428\n",
      "642 0.0012138515285300266\n",
      "643 0.0011834385988719334\n",
      "644 0.0037343870351463243\n",
      "645 0.0010469504459296505\n",
      "646 0.003597076087760066\n",
      "647 0.004665959791760382\n",
      "648 0.0013132524759028874\n",
      "649 0.0005122097780461555\n",
      "650 1.3112370327021121e-05\n",
      "651 0.0006272908270576654\n",
      "652 0.0007535116176281503\n",
      "653 0.0006102695273661693\n",
      "654 0.00030458193124302356\n",
      "655 0.001731511671811177\n",
      "656 0.0009219336002254442\n",
      "657 0.0015627083005036178\n",
      "658 0.0011338468333805794\n",
      "659 0.001017616912696325\n",
      "660 0.0018548861340866565\n",
      "661 0.00031727849251394206\n",
      "662 0.00040732244933733183\n",
      "663 0.0020314351062525452\n",
      "664 0.002048424405930153\n",
      "665 0.00041290930298995673\n",
      "666 0.0020550145050249736\n",
      "667 0.0016389873741660163\n",
      "668 0.0015034882282010285\n",
      "669 0.00039594238744343783\n",
      "670 0.00019196871450860255\n",
      "671 0.00018454341223529894\n",
      "672 0.0022827108678310924\n",
      "673 0.0023006715555888616\n",
      "674 0.0016767847445476804\n",
      "675 0.0015186529596484572\n",
      "676 0.0004987849606959378\n",
      "677 0.0010151351103448345\n",
      "678 0.0004820463179214155\n",
      "679 0.0003945540049115757\n",
      "680 0.00036300238215298923\n",
      "681 0.0011980412534273645\n",
      "682 0.0008711843747827246\n",
      "683 0.0018761843574037352\n",
      "684 0.0009952610328061737\n",
      "685 0.001655304413458065\n",
      "686 0.0026743446245032733\n",
      "687 0.0010148293321816553\n",
      "688 0.0007317546373707844\n",
      "689 0.0013441460953144693\n",
      "690 0.0010396119453372306\n",
      "691 0.0010160848472849078\n",
      "692 0.00132131760129443\n",
      "693 0.0016025604361908357\n",
      "694 0.0008401397049355595\n",
      "695 0.0005458610911415616\n",
      "696 0.0026022705553572674\n",
      "697 0.001967560237575352\n",
      "698 0.0025454115130286964\n",
      "699 0.0017209654723908793\n",
      "700 0.0018526121504240847\n",
      "701 0.0007648215393580758\n",
      "702 0.0009042370588755421\n",
      "703 0.001306951153752404\n",
      "704 0.0011105694817537721\n",
      "705 0.0017970281829260237\n",
      "706 0.000570770363866274\n",
      "707 0.00021147038876333623\n",
      "708 0.0011527050767735677\n",
      "709 0.0005674917165551323\n",
      "710 0.0006939854182619371\n",
      "711 0.0008040653814676388\n",
      "712 0.0012611125495110285\n",
      "713 0.002110839549691986\n",
      "714 0.0017357524936228322\n",
      "715 0.0007078996287985994\n",
      "716 0.0008413887249750043\n",
      "717 0.0017729404783297124\n",
      "718 0.000368532926467846\n",
      "719 0.00013233973627739963\n",
      "720 0.000183796279918486\n",
      "721 0.00014381056469881334\n",
      "722 0.0009493536136055837\n",
      "723 5.080313526778753e-05\n",
      "726 0.00030817154877842015\n",
      "727 9.389941780862947e-06\n",
      "729 0.0006516739813586896\n",
      "730 0.0002811223304656942\n",
      "731 7.693348596660537e-05\n",
      "732 2.7381230804768937e-05\n",
      "733 6.754082628078071e-06\n",
      "734 2.2999940499562688e-05\n",
      "735 0.0012901235909530435\n",
      "736 0.0006430115627674269\n",
      "737 0.00043163073445335736\n",
      "738 0.0011462893401444018\n",
      "739 0.0003170578131881956\n",
      "740 0.0002672212377127102\n",
      "741 0.00029230990889663583\n",
      "742 0.00010523783251648359\n",
      "743 0.0004200809657529754\n",
      "744 5.1213371646113196e-05\n",
      "745 2.9723581129191042e-06\n",
      "746 2.5513322029433794e-05\n",
      "747 0.00035549090553597647\n",
      "748 7.306353245962152e-05\n",
      "749 8.514001167232264e-07\n",
      "750 0.0002278011636985471\n",
      "751 0.0008078326188941656\n",
      "752 6.008154214266005e-05\n",
      "753 0.0005519509899238017\n",
      "754 2.890647901257655e-05\n",
      "755 2.1070526342742824e-06\n",
      "756 0.0013673403712633838\n",
      "757 0.0010864810514445867\n",
      "758 0.00045361094520065073\n",
      "759 0.0015168543003184266\n",
      "760 0.00039629093152678305\n",
      "761 4.7949669634325174e-05\n",
      "762 0.0006041595390783388\n",
      "763 0.0001413917205383284\n",
      "764 0.00020918599791757374\n",
      "765 4.3413093405680406e-05\n",
      "766 1.2265664239258862e-05\n",
      "767 5.114983857215973e-07\n",
      "768 0.0001502138466296724\n",
      "769 0.0002778826102344288\n",
      "771 0.0007547197755487981\n",
      "772 0.0002164977240008217\n",
      "773 1.4880974141798283e-05\n",
      "774 0.00013143322195307354\n",
      "777 0.0015916527434022307\n",
      "778 0.0011707316958413042\n",
      "779 0.0011698248987922252\n",
      "780 0.0014936593084189042\n",
      "781 0.0004263647682048377\n",
      "782 0.00022102061366638837\n",
      "783 0.0004037652544606752\n",
      "784 5.933612323276115e-05\n",
      "785 0.0007479333623958696\n",
      "786 6.534532352458243e-05\n",
      "789 0.00027848187278960217\n",
      "790 2.6973218357964774e-05\n",
      "791 1.3240780215584975e-06\n",
      "792 0.00044860989638729814\n",
      "793 0.00022503669316375447\n",
      "794 5.648967844011953e-05\n",
      "795 1.648787612683575e-05\n",
      "796 9.041702501567032e-06\n",
      "797 7.499898456023461e-06\n",
      "798 0.0014087979639249423\n",
      "799 0.0010141271391119868\n",
      "800 0.0005701380025266201\n",
      "801 0.0012817072221668355\n",
      "802 0.00036040989301623443\n",
      "803 0.00035383806567463\n",
      "804 0.00045094073457738177\n",
      "805 6.988450030973014e-05\n",
      "806 0.00041864239754241847\n",
      "807 3.3648024607685065e-05\n",
      "808 2.2156175297890247e-06\n",
      "809 1.5584110769610496e-05\n",
      "810 0.0001995435057101648\n",
      "811 0.00014383710602723133\n",
      "813 0.0002851898251386727\n",
      "814 0.0004842429183866818\n",
      "815 7.145183527613814e-05\n",
      "816 0.0008351088475024895\n",
      "817 2.4835520383727858e-05\n",
      "818 2.006130331764325e-05\n",
      "819 0.0015974637464458976\n",
      "820 0.001580229804854478\n",
      "821 0.0005963794565377627\n",
      "822 0.0017020652170487538\n",
      "823 0.0003469143716917162\n",
      "824 3.788960024268137e-05\n",
      "825 0.0009756619858494802\n",
      "826 0.00018351155824672865\n",
      "827 0.0003302613283062724\n",
      "828 5.014232101214174e-05\n",
      "829 1.3703410788781931e-05\n",
      "830 8.098668972251818e-06\n",
      "831 0.00010709221465580414\n",
      "832 0.00027275092483741633\n",
      "834 0.0006717674548988128\n",
      "835 0.00018281403054552835\n",
      "836 4.191028954848362e-05\n",
      "837 0.00011903308834931865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840 0.0005079055212452246\n",
      "841 0.00034755206265342574\n",
      "843 0.00039644667828254385\n",
      "846 0.00015281340922546822\n",
      "848 1.4302560750626046e-05\n",
      "861 0.0005942574025596039\n",
      "862 0.00029080139925572704\n",
      "864 0.00042358651602224684\n",
      "867 0.00010707238809097787\n",
      "869 2.5224611497949344e-05\n",
      "A 0.01468922825178363\n",
      "C 0.018588951699472948\n",
      "D 0.015579367835511565\n",
      "E 0.020929995128201026\n",
      "F 0.020106071621627798\n",
      "G 0.022263209597039767\n",
      "H 0.01329083008567742\n",
      "I 0.024219010841149222\n",
      "K 0.014172710565228636\n",
      "L 0.02408950161358666\n",
      "M 0.022251116854948114\n",
      "N 0.01856265670260251\n",
      "P 0.021946573762174636\n",
      "Q 0.01515895204305321\n",
      "R 0.015914218644019876\n",
      "S 0.01993105552862077\n",
      "T 0.01590470457053153\n",
      "V 0.013981620918629457\n",
      "W 0.0165905232958883\n",
      "Y 0.018816805567750297\n",
      "670\n"
     ]
    }
   ],
   "source": [
    "jumlah =0\n",
    "importance=[]\n",
    "for i in range(len(my_input_matrix.columns)):\n",
    "    if(clf.feature_importances_[i]>0):\n",
    "        print(my_input_matrix.columns[i],clf.feature_importances_[i])\n",
    "        importance.append(my_input_matrix.columns[i])\n",
    "        jumlah=jumlah+1\n",
    "print(jumlah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '7',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '62',\n",
       " '63',\n",
       " '66',\n",
       " '67',\n",
       " '83',\n",
       " '94',\n",
       " '95',\n",
       " '114',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '123',\n",
       " '124',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '137',\n",
       " '138',\n",
       " '140',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '158',\n",
       " '160',\n",
       " '161',\n",
       " '165',\n",
       " '167',\n",
       " '168',\n",
       " '172',\n",
       " '174',\n",
       " '175',\n",
       " '179',\n",
       " '180',\n",
       " '181',\n",
       " '182',\n",
       " '186',\n",
       " '187',\n",
       " '188',\n",
       " '189',\n",
       " '193',\n",
       " '194',\n",
       " '195',\n",
       " '196',\n",
       " '200',\n",
       " '201',\n",
       " '202',\n",
       " '203',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '210',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '217',\n",
       " '221',\n",
       " '224',\n",
       " '228',\n",
       " '229',\n",
       " '230',\n",
       " '231',\n",
       " '235',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '249',\n",
       " '250',\n",
       " '251',\n",
       " '252',\n",
       " '256',\n",
       " '257',\n",
       " '258',\n",
       " '259',\n",
       " '260',\n",
       " '261',\n",
       " '262',\n",
       " '263',\n",
       " '275',\n",
       " '276',\n",
       " '277',\n",
       " '278',\n",
       " '284',\n",
       " '285',\n",
       " '286',\n",
       " '287',\n",
       " '288',\n",
       " '293',\n",
       " '294',\n",
       " '295',\n",
       " '297',\n",
       " '298',\n",
       " '299',\n",
       " '300',\n",
       " '301',\n",
       " '302',\n",
       " '305',\n",
       " '306',\n",
       " '309',\n",
       " '310',\n",
       " '315',\n",
       " '317',\n",
       " '328',\n",
       " '329',\n",
       " '330',\n",
       " '331',\n",
       " '332',\n",
       " '333',\n",
       " '334',\n",
       " '335',\n",
       " '336',\n",
       " '337',\n",
       " '338',\n",
       " '339',\n",
       " '340',\n",
       " '341',\n",
       " '342',\n",
       " '343',\n",
       " '344',\n",
       " '345',\n",
       " '346',\n",
       " '347',\n",
       " '348',\n",
       " '349',\n",
       " '350',\n",
       " '351',\n",
       " '352',\n",
       " '353',\n",
       " '354',\n",
       " '355',\n",
       " '356',\n",
       " '357',\n",
       " '358',\n",
       " '359',\n",
       " '360',\n",
       " '361',\n",
       " '362',\n",
       " '363',\n",
       " '364',\n",
       " '365',\n",
       " '366',\n",
       " '367',\n",
       " '368',\n",
       " '369',\n",
       " '370',\n",
       " '371',\n",
       " '372',\n",
       " '373',\n",
       " '374',\n",
       " '375',\n",
       " '376',\n",
       " '377',\n",
       " '378',\n",
       " '379',\n",
       " '380',\n",
       " '381',\n",
       " '382',\n",
       " '383',\n",
       " '384',\n",
       " '385',\n",
       " '386',\n",
       " '387',\n",
       " '388',\n",
       " '389',\n",
       " '390',\n",
       " '391',\n",
       " '392',\n",
       " '393',\n",
       " '394',\n",
       " '395',\n",
       " '396',\n",
       " '397',\n",
       " '398',\n",
       " '399',\n",
       " '400',\n",
       " '401',\n",
       " '402',\n",
       " '403',\n",
       " '404',\n",
       " '405',\n",
       " '406',\n",
       " '407',\n",
       " '408',\n",
       " '409',\n",
       " '410',\n",
       " '411',\n",
       " '412',\n",
       " '413',\n",
       " '414',\n",
       " '415',\n",
       " '416',\n",
       " '417',\n",
       " '418',\n",
       " '419',\n",
       " '420',\n",
       " '421',\n",
       " '422',\n",
       " '423',\n",
       " '424',\n",
       " '426',\n",
       " '428',\n",
       " '429',\n",
       " '430',\n",
       " '431',\n",
       " '432',\n",
       " '433',\n",
       " '435',\n",
       " '436',\n",
       " '437',\n",
       " '438',\n",
       " '439',\n",
       " '440',\n",
       " '441',\n",
       " '442',\n",
       " '443',\n",
       " '444',\n",
       " '446',\n",
       " '447',\n",
       " '448',\n",
       " '449',\n",
       " '450',\n",
       " '451',\n",
       " '452',\n",
       " '453',\n",
       " '454',\n",
       " '455',\n",
       " '456',\n",
       " '457',\n",
       " '458',\n",
       " '459',\n",
       " '460',\n",
       " '461',\n",
       " '462',\n",
       " '463',\n",
       " '465',\n",
       " '466',\n",
       " '467',\n",
       " '468',\n",
       " '469',\n",
       " '471',\n",
       " '472',\n",
       " '473',\n",
       " '474',\n",
       " '475',\n",
       " '476',\n",
       " '477',\n",
       " '478',\n",
       " '479',\n",
       " '480',\n",
       " '481',\n",
       " '482',\n",
       " '483',\n",
       " '484',\n",
       " '485',\n",
       " '486',\n",
       " '487',\n",
       " '488',\n",
       " '489',\n",
       " '490',\n",
       " '491',\n",
       " '492',\n",
       " '493',\n",
       " '494',\n",
       " '495',\n",
       " '496',\n",
       " '497',\n",
       " '498',\n",
       " '499',\n",
       " '500',\n",
       " '501',\n",
       " '502',\n",
       " '503',\n",
       " '504',\n",
       " '505',\n",
       " '506',\n",
       " '507',\n",
       " '508',\n",
       " '509',\n",
       " '510',\n",
       " '511',\n",
       " '512',\n",
       " '514',\n",
       " '515',\n",
       " '516',\n",
       " '517',\n",
       " '518',\n",
       " '519',\n",
       " '520',\n",
       " '521',\n",
       " '522',\n",
       " '523',\n",
       " '524',\n",
       " '525',\n",
       " '527',\n",
       " '528',\n",
       " '529',\n",
       " '530',\n",
       " '531',\n",
       " '532',\n",
       " '533',\n",
       " '534',\n",
       " '535',\n",
       " '536',\n",
       " '537',\n",
       " '538',\n",
       " '539',\n",
       " '540',\n",
       " '541',\n",
       " '542',\n",
       " '543',\n",
       " '544',\n",
       " '545',\n",
       " '546',\n",
       " '547',\n",
       " '548',\n",
       " '549',\n",
       " '550',\n",
       " '551',\n",
       " '552',\n",
       " '553',\n",
       " '554',\n",
       " '555',\n",
       " '556',\n",
       " '557',\n",
       " '558',\n",
       " '559',\n",
       " '560',\n",
       " '561',\n",
       " '562',\n",
       " '563',\n",
       " '564',\n",
       " '565',\n",
       " '566',\n",
       " '567',\n",
       " '568',\n",
       " '569',\n",
       " '570',\n",
       " '571',\n",
       " '572',\n",
       " '573',\n",
       " '574',\n",
       " '575',\n",
       " '576',\n",
       " '577',\n",
       " '578',\n",
       " '579',\n",
       " '580',\n",
       " '581',\n",
       " '582',\n",
       " '583',\n",
       " '584',\n",
       " '585',\n",
       " '586',\n",
       " '587',\n",
       " '588',\n",
       " '589',\n",
       " '590',\n",
       " '591',\n",
       " '592',\n",
       " '593',\n",
       " '594',\n",
       " '595',\n",
       " '596',\n",
       " '597',\n",
       " '598',\n",
       " '599',\n",
       " '600',\n",
       " '601',\n",
       " '602',\n",
       " '603',\n",
       " '604',\n",
       " '605',\n",
       " '606',\n",
       " '607',\n",
       " '608',\n",
       " '609',\n",
       " '610',\n",
       " '611',\n",
       " '612',\n",
       " '613',\n",
       " '614',\n",
       " '615',\n",
       " '616',\n",
       " '617',\n",
       " '618',\n",
       " '619',\n",
       " '620',\n",
       " '621',\n",
       " '622',\n",
       " '623',\n",
       " '624',\n",
       " '625',\n",
       " '626',\n",
       " '627',\n",
       " '629',\n",
       " '630',\n",
       " '631',\n",
       " '632',\n",
       " '633',\n",
       " '634',\n",
       " '635',\n",
       " '636',\n",
       " '637',\n",
       " '638',\n",
       " '639',\n",
       " '640',\n",
       " '641',\n",
       " '642',\n",
       " '643',\n",
       " '644',\n",
       " '645',\n",
       " '646',\n",
       " '647',\n",
       " '648',\n",
       " '649',\n",
       " '650',\n",
       " '651',\n",
       " '652',\n",
       " '653',\n",
       " '654',\n",
       " '655',\n",
       " '656',\n",
       " '657',\n",
       " '658',\n",
       " '659',\n",
       " '660',\n",
       " '661',\n",
       " '662',\n",
       " '663',\n",
       " '664',\n",
       " '665',\n",
       " '666',\n",
       " '667',\n",
       " '668',\n",
       " '669',\n",
       " '670',\n",
       " '671',\n",
       " '672',\n",
       " '673',\n",
       " '674',\n",
       " '675',\n",
       " '676',\n",
       " '677',\n",
       " '678',\n",
       " '679',\n",
       " '680',\n",
       " '681',\n",
       " '682',\n",
       " '683',\n",
       " '684',\n",
       " '685',\n",
       " '686',\n",
       " '687',\n",
       " '688',\n",
       " '689',\n",
       " '690',\n",
       " '691',\n",
       " '692',\n",
       " '693',\n",
       " '694',\n",
       " '695',\n",
       " '696',\n",
       " '697',\n",
       " '698',\n",
       " '699',\n",
       " '700',\n",
       " '701',\n",
       " '702',\n",
       " '703',\n",
       " '704',\n",
       " '705',\n",
       " '706',\n",
       " '707',\n",
       " '708',\n",
       " '709',\n",
       " '710',\n",
       " '711',\n",
       " '712',\n",
       " '713',\n",
       " '714',\n",
       " '715',\n",
       " '716',\n",
       " '717',\n",
       " '718',\n",
       " '719',\n",
       " '720',\n",
       " '721',\n",
       " '722',\n",
       " '723',\n",
       " '726',\n",
       " '727',\n",
       " '729',\n",
       " '730',\n",
       " '731',\n",
       " '732',\n",
       " '733',\n",
       " '734',\n",
       " '735',\n",
       " '736',\n",
       " '737',\n",
       " '738',\n",
       " '739',\n",
       " '740',\n",
       " '741',\n",
       " '742',\n",
       " '743',\n",
       " '744',\n",
       " '745',\n",
       " '746',\n",
       " '747',\n",
       " '748',\n",
       " '749',\n",
       " '750',\n",
       " '751',\n",
       " '752',\n",
       " '753',\n",
       " '754',\n",
       " '755',\n",
       " '756',\n",
       " '757',\n",
       " '758',\n",
       " '759',\n",
       " '760',\n",
       " '761',\n",
       " '762',\n",
       " '763',\n",
       " '764',\n",
       " '765',\n",
       " '766',\n",
       " '767',\n",
       " '768',\n",
       " '769',\n",
       " '771',\n",
       " '772',\n",
       " '773',\n",
       " '774',\n",
       " '777',\n",
       " '778',\n",
       " '779',\n",
       " '780',\n",
       " '781',\n",
       " '782',\n",
       " '783',\n",
       " '784',\n",
       " '785',\n",
       " '786',\n",
       " '789',\n",
       " '790',\n",
       " '791',\n",
       " '792',\n",
       " '793',\n",
       " '794',\n",
       " '795',\n",
       " '796',\n",
       " '797',\n",
       " '798',\n",
       " '799',\n",
       " '800',\n",
       " '801',\n",
       " '802',\n",
       " '803',\n",
       " '804',\n",
       " '805',\n",
       " '806',\n",
       " '807',\n",
       " '808',\n",
       " '809',\n",
       " '810',\n",
       " '811',\n",
       " '813',\n",
       " '814',\n",
       " '815',\n",
       " '816',\n",
       " '817',\n",
       " '818',\n",
       " '819',\n",
       " '820',\n",
       " '821',\n",
       " '822',\n",
       " '823',\n",
       " '824',\n",
       " '825',\n",
       " '826',\n",
       " '827',\n",
       " '828',\n",
       " '829',\n",
       " '830',\n",
       " '831',\n",
       " '832',\n",
       " '834',\n",
       " '835',\n",
       " '836',\n",
       " '837',\n",
       " '840',\n",
       " '841',\n",
       " '843',\n",
       " '846',\n",
       " '848',\n",
       " '861',\n",
       " '862',\n",
       " '864',\n",
       " '867',\n",
       " '869',\n",
       " 'A',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = xgb.XGBClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost.fit(my_input_matrix,my_label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.000499967\n",
      "2 0.00024061424\n",
      "3 0.0020926378\n",
      "4 0.0050516175\n",
      "10 0.0001870047\n",
      "11 0.00028345615\n",
      "12 0.0010107459\n",
      "13 0.0018792743\n",
      "14 0.0019060015\n",
      "15 0.002003519\n",
      "16 0.0038042837\n",
      "17 0.003928135\n",
      "18 0.010616256\n",
      "19 0.0011956682\n",
      "20 0.0014132776\n",
      "21 0.0018067971\n",
      "22 0.0039562364\n",
      "23 9.346682e-05\n",
      "24 0.0031430572\n",
      "25 0.00087803195\n",
      "26 0.0003390835\n",
      "27 0.0025081642\n",
      "31 0.0018382323\n",
      "32 0.0021721171\n",
      "34 0.0010802329\n",
      "35 0.0010689694\n",
      "39 0.0020405664\n",
      "40 0.00010426917\n",
      "44 0.0013122582\n",
      "47 0.00013064194\n",
      "48 0.00058379426\n",
      "49 0.00011419448\n",
      "62 5.8438436e-05\n",
      "66 0.0008972882\n",
      "116 0.0005254278\n",
      "119 0.0010926235\n",
      "130 0.0004971905\n",
      "132 0.0003178554\n",
      "144 0.0007333353\n",
      "145 0.00089568103\n",
      "146 0.00063737936\n",
      "147 0.0032329955\n",
      "151 0.0029824488\n",
      "153 0.0047467873\n",
      "154 0.001575075\n",
      "158 0.0011483543\n",
      "160 0.00016941322\n",
      "179 0.0028719367\n",
      "180 0.00081575173\n",
      "181 0.002112575\n",
      "182 0.0015827573\n",
      "186 0.004104868\n",
      "187 0.0012230771\n",
      "188 0.0011907722\n",
      "189 0.0034714092\n",
      "193 0.0013965819\n",
      "194 0.001096572\n",
      "195 0.003840685\n",
      "200 0.0016830176\n",
      "201 0.00060400047\n",
      "207 0.0010262714\n",
      "208 0.00060507824\n",
      "214 0.00078184647\n",
      "215 7.790145e-05\n",
      "216 0.0019138891\n",
      "221 0.00032834377\n",
      "228 0.0021404247\n",
      "230 0.002329201\n",
      "231 0.00056204514\n",
      "242 0.0009560897\n",
      "244 0.001514164\n",
      "245 0.0003320961\n",
      "249 0.0020459725\n",
      "250 0.0026756383\n",
      "251 0.0008712229\n",
      "252 0.0009241727\n",
      "256 0.00066143577\n",
      "257 0.0015161764\n",
      "258 0.0020746426\n",
      "259 0.002071605\n",
      "260 0.000985271\n",
      "261 0.032685548\n",
      "262 0.00096020434\n",
      "263 0.0021240783\n",
      "285 0.00040872686\n",
      "286 0.00040753128\n",
      "287 0.00025005225\n",
      "288 0.0074497834\n",
      "294 0.001409657\n",
      "300 0.0029881552\n",
      "301 0.002699538\n",
      "302 0.0010712873\n",
      "306 0.0004679542\n",
      "309 0.0025937841\n",
      "315 0.0014615962\n",
      "333 0.00035206866\n",
      "334 0.0006754661\n",
      "335 0.000815137\n",
      "336 0.0023665451\n",
      "337 0.008187661\n",
      "338 0.001301998\n",
      "339 0.0066844267\n",
      "340 0.013111467\n",
      "341 0.0004652803\n",
      "342 0.00078774005\n",
      "346 0.0029008973\n",
      "347 0.0010041542\n",
      "348 0.0010781089\n",
      "350 0.0014465584\n",
      "353 0.0008716012\n",
      "354 0.0008576462\n",
      "356 0.0015275382\n",
      "357 0.0010635767\n",
      "358 0.0019087188\n",
      "359 0.00032814793\n",
      "360 0.0041537057\n",
      "363 0.00035136158\n",
      "364 0.0004209529\n",
      "365 0.00078096765\n",
      "366 0.0021549503\n",
      "367 0.0024228813\n",
      "368 0.0012250015\n",
      "369 0.00035160198\n",
      "371 0.00029445905\n",
      "373 0.0015059897\n",
      "374 0.004847629\n",
      "375 0.0016134974\n",
      "376 0.0012806585\n",
      "377 0.0015445232\n",
      "378 0.005660574\n",
      "379 0.002495829\n",
      "380 0.0021524932\n",
      "381 0.0011614914\n",
      "382 0.00400082\n",
      "383 0.0057761353\n",
      "384 0.0021902877\n",
      "385 0.0011600568\n",
      "386 0.0012448124\n",
      "387 0.00045382913\n",
      "389 0.0014144898\n",
      "390 0.004947032\n",
      "391 0.0005605511\n",
      "392 0.010456276\n",
      "393 0.0017013999\n",
      "394 0.005461793\n",
      "395 0.0011900692\n",
      "397 0.0024943694\n",
      "398 0.0017915475\n",
      "400 0.008175596\n",
      "401 0.0017189633\n",
      "402 0.00088947214\n",
      "404 0.0014217441\n",
      "406 0.003474106\n",
      "407 0.0014130139\n",
      "408 0.0009456084\n",
      "409 0.00018807729\n",
      "410 0.00056088413\n",
      "411 0.0039171996\n",
      "413 0.002647245\n",
      "415 0.0013324422\n",
      "417 0.0009967084\n",
      "418 0.00041937613\n",
      "419 0.00066098414\n",
      "420 0.009516481\n",
      "421 0.0008364281\n",
      "422 0.00027267405\n",
      "423 0.0012289999\n",
      "424 0.0013542649\n",
      "430 0.0038621086\n",
      "431 0.0012538725\n",
      "432 0.0017605678\n",
      "433 0.0017866812\n",
      "437 0.0014878155\n",
      "438 0.0013945911\n",
      "439 0.0009559243\n",
      "440 0.0014436279\n",
      "441 0.003792306\n",
      "443 0.00090617\n",
      "444 0.002033824\n",
      "446 0.0019902857\n",
      "447 0.0023595802\n",
      "448 0.0070413915\n",
      "450 0.00038577433\n",
      "451 0.004241293\n",
      "452 0.0006115746\n",
      "453 0.0013990967\n",
      "454 0.006431889\n",
      "456 0.0005703512\n",
      "459 0.0005075874\n",
      "460 0.0007925652\n",
      "462 0.00044462134\n",
      "463 0.0005949085\n",
      "465 0.0068372474\n",
      "466 0.0014184415\n",
      "467 0.0068444717\n",
      "468 0.0002492661\n",
      "475 0.0008802829\n",
      "477 0.0005399996\n",
      "478 0.0004155468\n",
      "480 0.0018839552\n",
      "483 0.0018177606\n",
      "485 0.00034084026\n",
      "486 0.0013384498\n",
      "487 0.00045781318\n",
      "488 0.0011961687\n",
      "489 0.0026505236\n",
      "490 0.0021609643\n",
      "491 0.0008480177\n",
      "492 0.0056528724\n",
      "494 0.0008311702\n",
      "495 0.0010148368\n",
      "496 0.0015648765\n",
      "497 0.0002563076\n",
      "498 0.0022843108\n",
      "499 0.0005304272\n",
      "500 0.0035603472\n",
      "501 0.0034098662\n",
      "503 0.0029456697\n",
      "504 0.00037216852\n",
      "505 0.0005576894\n",
      "508 0.00032925725\n",
      "509 0.0063183443\n",
      "512 0.005285771\n",
      "514 8.653139e-05\n",
      "515 0.0011117238\n",
      "516 0.00039476715\n",
      "517 0.0011195487\n",
      "518 0.00036621682\n",
      "519 0.0008452234\n",
      "520 0.00012016728\n",
      "522 0.00077602506\n",
      "523 0.0051513365\n",
      "524 0.0001607499\n",
      "525 0.0020130863\n",
      "528 0.0058086785\n",
      "529 0.007617706\n",
      "532 0.0006916496\n",
      "533 0.0004921497\n",
      "536 0.00096573366\n",
      "537 0.00220871\n",
      "539 0.0004604115\n",
      "540 0.0014088019\n",
      "541 0.0019532961\n",
      "543 0.0005507903\n",
      "544 0.00097319984\n",
      "545 0.001380478\n",
      "547 0.00024503836\n",
      "548 0.0027144034\n",
      "549 6.409489e-05\n",
      "550 0.0019049435\n",
      "554 0.002930779\n",
      "556 0.00039243963\n",
      "560 0.0054896595\n",
      "561 0.0012779378\n",
      "562 0.0044499915\n",
      "565 0.001287928\n",
      "567 0.00092360884\n",
      "568 0.0015252002\n",
      "569 0.011239821\n",
      "570 0.0015831587\n",
      "572 0.005110146\n",
      "573 0.0027591956\n",
      "574 0.00063076604\n",
      "576 0.0006601821\n",
      "577 0.00070582255\n",
      "578 0.0009130135\n",
      "580 0.0004450108\n",
      "581 0.00062807027\n",
      "582 0.00086281076\n",
      "583 0.0011421615\n",
      "586 0.003531286\n",
      "587 0.0007340294\n",
      "588 0.00017787238\n",
      "589 0.001256825\n",
      "591 0.0017772985\n",
      "593 0.0009001237\n",
      "594 0.0015724846\n",
      "595 0.00060609845\n",
      "597 0.0009833026\n",
      "598 0.0008481786\n",
      "599 0.02406642\n",
      "600 0.0020312744\n",
      "601 0.0008350887\n",
      "602 0.0005752845\n",
      "603 0.011401344\n",
      "605 0.0014997933\n",
      "606 0.0017817797\n",
      "608 0.0010946165\n",
      "610 0.00043281977\n",
      "611 0.00071095413\n",
      "612 0.0015003777\n",
      "613 0.004186471\n",
      "614 0.001130773\n",
      "615 0.0018054456\n",
      "616 0.0015138991\n",
      "617 0.0045226244\n",
      "618 0.0014039687\n",
      "620 0.0011007125\n",
      "621 0.000767541\n",
      "622 0.0010669382\n",
      "623 0.00017248811\n",
      "624 0.0009885705\n",
      "626 0.0009664416\n",
      "629 0.0013052843\n",
      "630 0.00011825802\n",
      "631 0.000113724906\n",
      "632 0.00057668745\n",
      "634 0.0009275432\n",
      "636 0.0013588207\n",
      "637 0.005798605\n",
      "638 0.00018720237\n",
      "639 0.0008919074\n",
      "640 0.00083427865\n",
      "642 0.0007180051\n",
      "644 0.0046386197\n",
      "645 0.00036230826\n",
      "646 0.0026770649\n",
      "647 0.0026038333\n",
      "648 0.0036766294\n",
      "651 0.00013507566\n",
      "653 0.0011114368\n",
      "654 0.00094376423\n",
      "656 0.0010991903\n",
      "657 0.0007805506\n",
      "659 0.0021520315\n",
      "660 0.0010990043\n",
      "663 0.02573617\n",
      "664 0.0022963916\n",
      "665 0.0015043601\n",
      "666 0.0015089043\n",
      "668 0.0010256171\n",
      "671 0.015849618\n",
      "672 0.0034690034\n",
      "673 0.0008334691\n",
      "674 0.00087685976\n",
      "675 0.0022978517\n",
      "677 0.0026236677\n",
      "680 0.0012734806\n",
      "681 0.000426747\n",
      "682 0.0014732196\n",
      "683 0.00083137053\n",
      "684 0.0008857029\n",
      "685 0.001798458\n",
      "686 0.0026411284\n",
      "687 0.00053442863\n",
      "688 0.0058905645\n",
      "689 0.0012357385\n",
      "690 0.0007562974\n",
      "691 0.0010257381\n",
      "692 0.0022525545\n",
      "693 0.00013746944\n",
      "694 0.0012312742\n",
      "695 0.00092320755\n",
      "696 0.002008031\n",
      "697 0.0030792316\n",
      "698 0.0014124762\n",
      "699 0.00082807196\n",
      "700 0.0014106684\n",
      "701 0.0002491913\n",
      "702 0.0010401215\n",
      "703 0.0022113388\n",
      "704 0.0024404272\n",
      "705 0.000718118\n",
      "706 0.0018951952\n",
      "707 0.00069364416\n",
      "708 0.0023171506\n",
      "709 0.0007170529\n",
      "710 0.0019349218\n",
      "711 0.00061033724\n",
      "712 0.0024074344\n",
      "713 0.000906563\n",
      "714 0.0026195731\n",
      "715 0.0008451674\n",
      "716 0.006395327\n",
      "717 0.0014453747\n",
      "718 0.0006226491\n",
      "719 0.0070826127\n",
      "720 0.0011821984\n",
      "722 0.0010187329\n",
      "726 0.0024471357\n",
      "727 7.036562e-05\n",
      "729 0.002135884\n",
      "731 0.00015145398\n",
      "735 0.0010280409\n",
      "736 0.0012720864\n",
      "737 0.0019693149\n",
      "738 0.0013239488\n",
      "739 0.0003323476\n",
      "740 0.00681112\n",
      "742 0.00014408694\n",
      "743 0.00023662005\n",
      "744 0.00032828713\n",
      "747 0.0013159747\n",
      "748 0.00091596914\n",
      "750 0.00023738162\n",
      "751 0.0019604175\n",
      "753 0.004189284\n",
      "756 0.0010619173\n",
      "758 0.004089187\n",
      "759 0.0016952218\n",
      "760 0.0014016277\n",
      "761 4.898277e-05\n",
      "763 0.0029094426\n",
      "764 0.0003095815\n",
      "768 0.00020156238\n",
      "769 0.0025362014\n",
      "771 0.007256929\n",
      "772 8.284633e-05\n",
      "777 0.0010386632\n",
      "778 0.005416268\n",
      "780 0.0009038722\n",
      "783 0.0009676294\n",
      "785 0.0007102598\n",
      "792 0.00012548997\n",
      "798 0.00038336538\n",
      "799 0.0016369277\n",
      "801 0.0011676358\n",
      "804 0.00017228967\n",
      "806 0.00012471055\n",
      "819 0.0013668949\n",
      "820 0.007860108\n",
      "822 0.0013094825\n",
      "825 0.00068501255\n",
      "827 0.0010866927\n",
      "834 0.00011124088\n",
      "840 0.0024614634\n",
      "841 0.00037449185\n",
      "846 0.00096795225\n",
      "862 0.0017277234\n",
      "864 0.0048058154\n",
      "867 0.0007180834\n",
      "A 0.006924496\n",
      "C 0.004117649\n",
      "D 0.0012763544\n",
      "E 0.005940804\n",
      "F 0.001154129\n",
      "G 0.010519224\n",
      "H 0.0016026315\n",
      "I 0.0020612008\n",
      "K 0.0010127211\n",
      "L 0.028930027\n",
      "M 0.028747741\n",
      "N 0.0014530682\n",
      "P 0.005779212\n",
      "Q 0.0032077094\n",
      "R 0.0058325483\n",
      "S 0.002626536\n",
      "T 0.001723523\n",
      "V 0.0021041543\n",
      "W 0.00128765\n",
      "Y 0.0011190325\n",
      "451\n"
     ]
    }
   ],
   "source": [
    "jumlah2 =0\n",
    "importance2=[]\n",
    "for i in range(len(my_input_matrix.columns)):\n",
    "    if(xgboost.feature_importances_[i]>0):\n",
    "        print(my_input_matrix.columns[i],xgboost.feature_importances_[i])\n",
    "        importance2.append(my_input_matrix.columns[i])\n",
    "        jumlah2=jumlah2+1\n",
    "print(jumlah2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '31',\n",
       " '32',\n",
       " '34',\n",
       " '35',\n",
       " '39',\n",
       " '40',\n",
       " '44',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '62',\n",
       " '66',\n",
       " '116',\n",
       " '119',\n",
       " '130',\n",
       " '132',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '151',\n",
       " '153',\n",
       " '154',\n",
       " '158',\n",
       " '160',\n",
       " '179',\n",
       " '180',\n",
       " '181',\n",
       " '182',\n",
       " '186',\n",
       " '187',\n",
       " '188',\n",
       " '189',\n",
       " '193',\n",
       " '194',\n",
       " '195',\n",
       " '200',\n",
       " '201',\n",
       " '207',\n",
       " '208',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '221',\n",
       " '228',\n",
       " '230',\n",
       " '231',\n",
       " '242',\n",
       " '244',\n",
       " '245',\n",
       " '249',\n",
       " '250',\n",
       " '251',\n",
       " '252',\n",
       " '256',\n",
       " '257',\n",
       " '258',\n",
       " '259',\n",
       " '260',\n",
       " '261',\n",
       " '262',\n",
       " '263',\n",
       " '285',\n",
       " '286',\n",
       " '287',\n",
       " '288',\n",
       " '294',\n",
       " '300',\n",
       " '301',\n",
       " '302',\n",
       " '306',\n",
       " '309',\n",
       " '315',\n",
       " '333',\n",
       " '334',\n",
       " '335',\n",
       " '336',\n",
       " '337',\n",
       " '338',\n",
       " '339',\n",
       " '340',\n",
       " '341',\n",
       " '342',\n",
       " '346',\n",
       " '347',\n",
       " '348',\n",
       " '350',\n",
       " '353',\n",
       " '354',\n",
       " '356',\n",
       " '357',\n",
       " '358',\n",
       " '359',\n",
       " '360',\n",
       " '363',\n",
       " '364',\n",
       " '365',\n",
       " '366',\n",
       " '367',\n",
       " '368',\n",
       " '369',\n",
       " '371',\n",
       " '373',\n",
       " '374',\n",
       " '375',\n",
       " '376',\n",
       " '377',\n",
       " '378',\n",
       " '379',\n",
       " '380',\n",
       " '381',\n",
       " '382',\n",
       " '383',\n",
       " '384',\n",
       " '385',\n",
       " '386',\n",
       " '387',\n",
       " '389',\n",
       " '390',\n",
       " '391',\n",
       " '392',\n",
       " '393',\n",
       " '394',\n",
       " '395',\n",
       " '397',\n",
       " '398',\n",
       " '400',\n",
       " '401',\n",
       " '402',\n",
       " '404',\n",
       " '406',\n",
       " '407',\n",
       " '408',\n",
       " '409',\n",
       " '410',\n",
       " '411',\n",
       " '413',\n",
       " '415',\n",
       " '417',\n",
       " '418',\n",
       " '419',\n",
       " '420',\n",
       " '421',\n",
       " '422',\n",
       " '423',\n",
       " '424',\n",
       " '430',\n",
       " '431',\n",
       " '432',\n",
       " '433',\n",
       " '437',\n",
       " '438',\n",
       " '439',\n",
       " '440',\n",
       " '441',\n",
       " '443',\n",
       " '444',\n",
       " '446',\n",
       " '447',\n",
       " '448',\n",
       " '450',\n",
       " '451',\n",
       " '452',\n",
       " '453',\n",
       " '454',\n",
       " '456',\n",
       " '459',\n",
       " '460',\n",
       " '462',\n",
       " '463',\n",
       " '465',\n",
       " '466',\n",
       " '467',\n",
       " '468',\n",
       " '475',\n",
       " '477',\n",
       " '478',\n",
       " '480',\n",
       " '483',\n",
       " '485',\n",
       " '486',\n",
       " '487',\n",
       " '488',\n",
       " '489',\n",
       " '490',\n",
       " '491',\n",
       " '492',\n",
       " '494',\n",
       " '495',\n",
       " '496',\n",
       " '497',\n",
       " '498',\n",
       " '499',\n",
       " '500',\n",
       " '501',\n",
       " '503',\n",
       " '504',\n",
       " '505',\n",
       " '508',\n",
       " '509',\n",
       " '512',\n",
       " '514',\n",
       " '515',\n",
       " '516',\n",
       " '517',\n",
       " '518',\n",
       " '519',\n",
       " '520',\n",
       " '522',\n",
       " '523',\n",
       " '524',\n",
       " '525',\n",
       " '528',\n",
       " '529',\n",
       " '532',\n",
       " '533',\n",
       " '536',\n",
       " '537',\n",
       " '539',\n",
       " '540',\n",
       " '541',\n",
       " '543',\n",
       " '544',\n",
       " '545',\n",
       " '547',\n",
       " '548',\n",
       " '549',\n",
       " '550',\n",
       " '554',\n",
       " '556',\n",
       " '560',\n",
       " '561',\n",
       " '562',\n",
       " '565',\n",
       " '567',\n",
       " '568',\n",
       " '569',\n",
       " '570',\n",
       " '572',\n",
       " '573',\n",
       " '574',\n",
       " '576',\n",
       " '577',\n",
       " '578',\n",
       " '580',\n",
       " '581',\n",
       " '582',\n",
       " '583',\n",
       " '586',\n",
       " '587',\n",
       " '588',\n",
       " '589',\n",
       " '591',\n",
       " '593',\n",
       " '594',\n",
       " '595',\n",
       " '597',\n",
       " '598',\n",
       " '599',\n",
       " '600',\n",
       " '601',\n",
       " '602',\n",
       " '603',\n",
       " '605',\n",
       " '606',\n",
       " '608',\n",
       " '610',\n",
       " '611',\n",
       " '612',\n",
       " '613',\n",
       " '614',\n",
       " '615',\n",
       " '616',\n",
       " '617',\n",
       " '618',\n",
       " '620',\n",
       " '621',\n",
       " '622',\n",
       " '623',\n",
       " '624',\n",
       " '626',\n",
       " '629',\n",
       " '630',\n",
       " '631',\n",
       " '632',\n",
       " '634',\n",
       " '636',\n",
       " '637',\n",
       " '638',\n",
       " '639',\n",
       " '640',\n",
       " '642',\n",
       " '644',\n",
       " '645',\n",
       " '646',\n",
       " '647',\n",
       " '648',\n",
       " '651',\n",
       " '653',\n",
       " '654',\n",
       " '656',\n",
       " '657',\n",
       " '659',\n",
       " '660',\n",
       " '663',\n",
       " '664',\n",
       " '665',\n",
       " '666',\n",
       " '668',\n",
       " '671',\n",
       " '672',\n",
       " '673',\n",
       " '674',\n",
       " '675',\n",
       " '677',\n",
       " '680',\n",
       " '681',\n",
       " '682',\n",
       " '683',\n",
       " '684',\n",
       " '685',\n",
       " '686',\n",
       " '687',\n",
       " '688',\n",
       " '689',\n",
       " '690',\n",
       " '691',\n",
       " '692',\n",
       " '693',\n",
       " '694',\n",
       " '695',\n",
       " '696',\n",
       " '697',\n",
       " '698',\n",
       " '699',\n",
       " '700',\n",
       " '701',\n",
       " '702',\n",
       " '703',\n",
       " '704',\n",
       " '705',\n",
       " '706',\n",
       " '707',\n",
       " '708',\n",
       " '709',\n",
       " '710',\n",
       " '711',\n",
       " '712',\n",
       " '713',\n",
       " '714',\n",
       " '715',\n",
       " '716',\n",
       " '717',\n",
       " '718',\n",
       " '719',\n",
       " '720',\n",
       " '722',\n",
       " '726',\n",
       " '727',\n",
       " '729',\n",
       " '731',\n",
       " '735',\n",
       " '736',\n",
       " '737',\n",
       " '738',\n",
       " '739',\n",
       " '740',\n",
       " '742',\n",
       " '743',\n",
       " '744',\n",
       " '747',\n",
       " '748',\n",
       " '750',\n",
       " '751',\n",
       " '753',\n",
       " '756',\n",
       " '758',\n",
       " '759',\n",
       " '760',\n",
       " '761',\n",
       " '763',\n",
       " '764',\n",
       " '768',\n",
       " '769',\n",
       " '771',\n",
       " '772',\n",
       " '777',\n",
       " '778',\n",
       " '780',\n",
       " '783',\n",
       " '785',\n",
       " '792',\n",
       " '798',\n",
       " '799',\n",
       " '801',\n",
       " '804',\n",
       " '806',\n",
       " '819',\n",
       " '820',\n",
       " '822',\n",
       " '825',\n",
       " '827',\n",
       " '834',\n",
       " '840',\n",
       " '841',\n",
       " '846',\n",
       " '862',\n",
       " '864',\n",
       " '867',\n",
       " 'A',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selected2=dataset[importance2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy=0.2)\n",
    "# X_res, y_res = rus.fit_resample(feature_selected1,my_label_vector)\n",
    "X_res2, y_res2 = rus.fit_resample(feature_selected2,my_label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24540, 670) (24540,)\n",
      "0    20450\n",
      "1     4090\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_res.shape, y_res.shape)\n",
    "print(pd.value_counts(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaksi_notfound_sisa = feature_selected2[~feature_selected2.isin(X_res)].dropna().reset_index(drop = True)\n",
    "interaksi_notfound_sisa_class = [0]*interaksi_notfound_sisa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016334</td>\n",
       "      <td>0.045372</td>\n",
       "      <td>0.027223</td>\n",
       "      <td>0.036298</td>\n",
       "      <td>0.047187</td>\n",
       "      <td>0.098004</td>\n",
       "      <td>0.059891</td>\n",
       "      <td>0.043557</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>0.032668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.038409</td>\n",
       "      <td>0.048011</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.009602</td>\n",
       "      <td>0.038409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030755</td>\n",
       "      <td>0.052190</td>\n",
       "      <td>0.037279</td>\n",
       "      <td>0.027959</td>\n",
       "      <td>0.051258</td>\n",
       "      <td>0.065238</td>\n",
       "      <td>0.068034</td>\n",
       "      <td>0.060578</td>\n",
       "      <td>0.012116</td>\n",
       "      <td>0.047530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034234</td>\n",
       "      <td>0.037838</td>\n",
       "      <td>0.055856</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.055856</td>\n",
       "      <td>0.055856</td>\n",
       "      <td>0.050450</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>0.039640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.041562</td>\n",
       "      <td>0.060453</td>\n",
       "      <td>0.051637</td>\n",
       "      <td>0.057935</td>\n",
       "      <td>0.076826</td>\n",
       "      <td>0.069270</td>\n",
       "      <td>0.073048</td>\n",
       "      <td>0.021411</td>\n",
       "      <td>0.027708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 670 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  7  10  11  12  13  14  ...         M         N         P  \\\n",
       "0  1  1  1  0  0   1   1   1   1   0  ...  0.016334  0.045372  0.027223   \n",
       "1  1  1  1  0  0   1   1   1   1   0  ...  0.030178  0.041152  0.032922   \n",
       "2  1  1  1  0  0   1   1   1   1   0  ...  0.030755  0.052190  0.037279   \n",
       "3  1  1  1  1  0   1   1   1   1   1  ...  0.034234  0.037838  0.055856   \n",
       "4  1  1  1  0  0   1   1   1   1   0  ...  0.010076  0.041562  0.060453   \n",
       "\n",
       "          Q         R         S         T         V         W         Y  \n",
       "0  0.036298  0.047187  0.098004  0.059891  0.043557  0.005445  0.032668  \n",
       "1  0.038409  0.048011  0.057613  0.061728  0.061728  0.009602  0.038409  \n",
       "2  0.027959  0.051258  0.065238  0.068034  0.060578  0.012116  0.047530  \n",
       "3  0.028829  0.055856  0.055856  0.050450  0.066667  0.025225  0.039640  \n",
       "4  0.051637  0.057935  0.076826  0.069270  0.073048  0.021411  0.027708  \n",
       "\n",
       "[5 rows x 670 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rus = pd.DataFrame(X_res, columns = importance)\n",
    "df_rus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Val Test split dari hasil ekstraksi fitur RF\n",
    "from sklearn.model_selection import train_test_split\n",
    "def train_validation_test_split(\n",
    "    X, y, train_size=0.8, val_size=0.1, test_size=0.1, \n",
    "    random_state=None, shuffle=True):\n",
    "    assert int(train_size + val_size + test_size + 1e-7) == 1\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, shuffle=shuffle)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val,    test_size=val_size/(train_size+val_size), \n",
    "        random_state=random_state, shuffle=shuffle)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n",
    "    X_res, y_res, train_size=0.8, val_size=0.1, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2083\n",
      "           1       0.94      0.93      0.94       371\n",
      "\n",
      "    accuracy                           0.98      2454\n",
      "   macro avg       0.96      0.96      0.96      2454\n",
      "weighted avg       0.98      0.98      0.98      2454\n",
      "\n",
      "accuracy 0.980440097799511\n",
      "ROC AUC score: 0.9607863942866979\n"
     ]
    }
   ],
   "source": [
    "#Build model XGB dari hasil ekstraksi fitur RF\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "model = xgb.XGBClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "#Predict on validation\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print('accuracy', accuracy_score(y_val, y_pred))\n",
    "print('ROC AUC score:',roc_auc_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2063\n",
      "           1       0.94      0.93      0.94       391\n",
      "\n",
      "    accuracy                           0.98      2454\n",
      "   macro avg       0.96      0.96      0.96      2454\n",
      "weighted avg       0.98      0.98      0.98      2454\n",
      "\n",
      "Accuracy 0.9800325998370009\n",
      "ROC AUC score: 0.9601411050626494\n"
     ]
    }
   ],
   "source": [
    "#Predict on test \n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy', accuracy_score(y_test, y_pred))\n",
    "print('ROC AUC score:', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Val Test split dari hasil ekstraksi fitur XGB\n",
    "from sklearn.model_selection import train_test_split\n",
    "def train_validation_test_split(\n",
    "    X, y, train_size=0.7, val_size=0.1, test_size=0.2, \n",
    "    random_state=None, shuffle=True):\n",
    "    assert int(train_size + val_size + test_size + 1e-7) == 1\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, shuffle=shuffle)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val,    test_size=val_size/(train_size+val_size), \n",
    "        random_state=random_state, shuffle=shuffle)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n",
    "    X_res2, y_res2, train_size=0.7, val_size=0.1, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2083\n",
      "           1       0.93      0.93      0.93       371\n",
      "\n",
      "    accuracy                           0.98      2454\n",
      "   macro avg       0.96      0.96      0.96      2454\n",
      "weighted avg       0.98      0.98      0.98      2454\n",
      "\n",
      "accuracy 0.9796251018744906\n",
      "ROC AUC score: 0.9603063174744078\n"
     ]
    }
   ],
   "source": [
    "#Build model XGB dari hasil ekstraksi fitur XGB\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "model = xgb.XGBClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "#Predict on validation\n",
    "print(classification_report(y_val, y_pred))\n",
    "print('accuracy', accuracy_score(y_val, y_pred))\n",
    "print('ROC AUC score:',roc_auc_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2063\n",
      "           1       0.94      0.94      0.94       391\n",
      "\n",
      "    accuracy                           0.98      2454\n",
      "   macro avg       0.96      0.96      0.96      2454\n",
      "weighted avg       0.98      0.98      0.98      2454\n",
      "\n",
      "Accuracy 0.980440097799511\n",
      "ROC AUC score: 0.9624562843325278\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy', accuracy_score(y_test, y_pred))\n",
    "print('ROC AUC score:', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    138398\n",
      "           1       0.73      0.98      0.84      4090\n",
      "\n",
      "    accuracy                           0.99    142488\n",
      "   macro avg       0.87      0.99      0.92    142488\n",
      "weighted avg       0.99      0.99      0.99    142488\n",
      "\n",
      "Accuracy: 0.989227162989164\n",
      "ROC AUC score: 0.9869802925837609\n"
     ]
    }
   ],
   "source": [
    "#apply to original datasets\n",
    "y_pred = model.predict(feature_selected2)\n",
    "print(classification_report(my_label_vector, y_pred))\n",
    "print('Accuracy:', accuracy_score(my_label_vector, y_pred))\n",
    "print('ROC AUC score:', roc_auc_score(my_label_vector, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2083\n",
      "           1       0.94      0.93      0.93       371\n",
      "\n",
      "    accuracy                           0.98      2454\n",
      "   macro avg       0.96      0.96      0.96      2454\n",
      "weighted avg       0.98      0.98      0.98      2454\n",
      "\n",
      "accuracy 0.9796251018744906\n",
      "ROC AUC score: 0.9580909764969403\n"
     ]
    }
   ],
   "source": [
    "model2 = RandomForestClassifier(random_state=0)\n",
    "model2.fit(X_train, y_train)\n",
    "#Predict on validation\n",
    "y_pred = model2.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print('accuracy', accuracy_score(y_val, y_pred))\n",
    "print('ROC AUC score:',roc_auc_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2063\n",
      "           1       0.95      0.94      0.95       391\n",
      "\n",
      "    accuracy                           0.98      2454\n",
      "   macro avg       0.97      0.97      0.97      2454\n",
      "weighted avg       0.98      0.98      0.98      2454\n",
      "\n",
      "Accuracy 0.9837000814995925\n",
      "ROC AUC score: 0.967504428903851\n"
     ]
    }
   ],
   "source": [
    "#Predict on test \n",
    "y_pred = model2.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy', accuracy_score(y_test, y_pred))\n",
    "print('ROC AUC score:', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    138398\n",
      "           1       0.78      0.99      0.87      4090\n",
      "\n",
      "    accuracy                           0.99    142488\n",
      "   macro avg       0.89      0.99      0.93    142488\n",
      "weighted avg       0.99      0.99      0.99    142488\n",
      "\n",
      "Accuracy: 0.9915571837628432\n",
      "ROC AUC score: 0.9894847346996232\n"
     ]
    }
   ],
   "source": [
    "#apply to original datasets\n",
    "y_pred = model2.predict(feature_selected2)\n",
    "print(classification_report(my_label_vector, y_pred))\n",
    "print('Accuracy:', accuracy_score(my_label_vector, y_pred))\n",
    "print('ROC AUC score:', roc_auc_score(my_label_vector, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF WIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  46 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-3)]: Done  50 out of  50 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200}\n",
      "0.9325503097408578\n"
     ]
    }
   ],
   "source": [
    "#Tune Parameter for RF\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# number of trees \n",
    "n_estimators = [int(x) for x in np.linspace(200,300,10)]\n",
    "#max depth\n",
    "max_depth = [int(x) for x in np.linspace(10,100,10)]\n",
    "#max feature\n",
    "max_features = [int(x) for x in np.linspace(25,35,10)]\n",
    "# create  grid\n",
    "param_grid = {\n",
    "'n_estimators': n_estimators,\n",
    "# 'max_depth': max_depth,\n",
    "# 'max_features': max_features,\n",
    " }\n",
    "scoring = {'ACC':'accuracy','F1':'f1', 'ROCAUC':'roc_auc'}\n",
    "\n",
    "# Grid search of parameters\n",
    "rf_tune = GridSearchCV(estimator = model3, param_grid = param_grid, cv = 5, scoring=scoring, n_jobs=-3, verbose=1, refit='F1')\n",
    "# Fit the model\n",
    "rf_tune.fit(X_train, y_train)\n",
    "# print results\n",
    "print(rf_tune.best_params_)\n",
    "print(rf_tune.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Result of 10 CV\n",
      "Accuracy    : 0.98024Â±0.002\n",
      "Recall      : 0.92543Â±0.008\n",
      "Precision   : 0.95466Â±0.006\n",
      "ROC-AUC     : 0.95831Â±0.004\n",
      "F1 Score    : 0.93978Â±0.005\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "#Stratified K-fold untuk RF di data Random Undersampling hasil ekstraksi fitur XGB\n",
    "\n",
    "\n",
    "res_all = [[],[],[],[],[]]\n",
    "auc_plots = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42,shuffle=True)\n",
    "for train_ind, test_ind in cv.split(X_res2, y_res2):\n",
    "  #Train the model\n",
    "  X_train,y_train = X_res2.iloc[train_ind,:],y_res2[train_ind]\n",
    "  X_test,y_test = X_res2.iloc[test_ind,:],y_res2[test_ind]\n",
    "\n",
    "  #Fitting model\n",
    "  RF_tuned = RandomForestClassifier(max_depth=30, max_features=32, n_estimators = 200, random_state=123)\n",
    "  RF_tuned.fit(X_train,y_train)\n",
    "\n",
    "  #Predict\n",
    "  y_predict_cv = RF_tuned.predict(X_test)\n",
    "\n",
    "  #Calculate metrics\n",
    "  accu = accuracy_score(y_test, y_predict_cv)\n",
    "  auc = roc_auc_score(y_test, y_predict_cv)\n",
    "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_predict_cv, average='binary',pos_label=1)\n",
    "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_predict_cv, average='binary',pos_label=0)\n",
    "\n",
    "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
    "  fpr, tpr, _ = roc_curve(y_test,  y_predict_cv)\n",
    "  auc_plots.append([fpr,tpr,auc])\n",
    " \n",
    "#Average and Stdv of k-fold CV\n",
    "print('Average Result of {} CV'.format(10))\n",
    "print('Accuracy    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
    "print('Recall      : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
    "print('Precision   : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
    "print('ROC-AUC     : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
    "print('F1 Score    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
    "print('===================================')\n",
    "\n",
    "#Choose auc plot with highest score\n",
    "best_auc_dnn = auc_plots[np.array(res_all[3]).argmax()]\n",
    "res_all_dnn = res_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    138398\n",
      "           1       0.79      0.99      0.88      4090\n",
      "\n",
      "    accuracy                           0.99    142488\n",
      "   macro avg       0.89      0.99      0.94    142488\n",
      "weighted avg       0.99      0.99      0.99    142488\n",
      "\n",
      "Accuracy: 0.9920203806636348\n",
      "ROC AUC score: 0.9924518197066814\n",
      "precision_recall_fscore (0.9478908188585607, 0.9339853300733496, 0.940886699507389, None)\n"
     ]
    }
   ],
   "source": [
    "#predict original datasets\n",
    "y_pred = RF_tuned.predict(feature_selected2)\n",
    "print(classification_report(my_label_vector, y_pred))\n",
    "print('Accuracy:', accuracy_score(my_label_vector, y_pred))\n",
    "print('ROC AUC score:', roc_auc_score(my_label_vector, y_pred))\n",
    "print('precision_recall_fscore',precision_recall_fscore_support(y_test, y_predict_cv, average='binary',pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137290,   1108],\n",
       "       [    29,   4061]], dtype=int64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(my_label_vector,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "fpr, tpr, threshold = roc_curve(my_label_vector,y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TESTING ANOTHER METHOD AS BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Result of 10 CV\n",
      "Accuracy    : 0.97339Â±0.003\n",
      "Recall      : 0.91711Â±0.013\n",
      "Precision   : 0.92286Â±0.010\n",
      "ROC-AUC     : 0.95088Â±0.007\n",
      "F1 Score    : 0.91990Â±0.008\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "#LGBM\n",
    "import lightgbm as lgb\n",
    "res_all = [[],[],[],[],[]]\n",
    "auc_plots = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42,shuffle=True)\n",
    "for train_ind, test_ind in cv.split(X_res2, y_res2):\n",
    "  #Train the model\n",
    "  X_train,y_train = X_res2.iloc[train_ind,:],y_res2[train_ind]\n",
    "  X_test,y_test = X_res2.iloc[test_ind,:],y_res2[test_ind]\n",
    "\n",
    "  #Fitting model\n",
    "  lgbc = lgb.LGBMClassifier(random_state = 123)\n",
    "  lgbc.fit(X_train,y_train)\n",
    "\n",
    "  #Predict\n",
    "  y_predict_lgbc = lgbc.predict(X_test)\n",
    "\n",
    "  #Calculate metrics\n",
    "  accu = accuracy_score(y_test, y_predict_lgbc)\n",
    "  auc = roc_auc_score(y_test, y_predict_lgbc)\n",
    "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_predict_lgbc, average='binary',pos_label=1)\n",
    "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_predict_lgbc, average='binary',pos_label=0)\n",
    "\n",
    "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
    "  fpr, tpr, _ = roc_curve(y_test,  y_predict_lgbc)\n",
    "  auc_plots.append([fpr,tpr,auc])\n",
    " \n",
    "#Average and Stdv of k-fold CV\n",
    "print('Average Result of {} CV'.format(10))\n",
    "print('Accuracy    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
    "print('Recall      : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
    "print('Precision   : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
    "print('ROC-AUC     : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
    "print('F1 Score    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
    "print('===================================')\n",
    "\n",
    "#Choose auc plot with highest score\n",
    "best_auc_dnn = auc_plots[np.array(res_all[3]).argmax()]\n",
    "res_all_dnn = res_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Result of 10 CV\n",
      "Accuracy    : 0.80383Â±0.009\n",
      "Recall      : 0.41051Â±0.049\n",
      "Precision   : 0.41086Â±0.026\n",
      "ROC-AUC     : 0.64650Â±0.023\n",
      "F1 Score    : 0.40993Â±0.034\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "#NAIVE BAYES\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "res_all = [[],[],[],[],[]]\n",
    "auc_plots = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42,shuffle=True)\n",
    "for train_ind, test_ind in cv.split(X_res2, y_res2):\n",
    "  #Train the model\n",
    "  X_train,y_train = X_res2.iloc[train_ind,:],y_res2[train_ind]\n",
    "  X_test,y_test = X_res2.iloc[test_ind,:],y_res2[test_ind]\n",
    "\n",
    "  #Fitting model\n",
    "  NB = GaussianNB()\n",
    "  NB.fit(X_train,y_train)\n",
    "\n",
    "  #Predict\n",
    "  y_predict_nb = NB.predict(X_test)\n",
    "\n",
    "  #Calculate metrics\n",
    "  accu = accuracy_score(y_test, y_predict_nb)\n",
    "  auc = roc_auc_score(y_test, y_predict_nb)\n",
    "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_predict_nb, average='binary',pos_label=1)\n",
    "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_predict_nb, average='binary',pos_label=0)\n",
    "\n",
    "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
    "  fpr, tpr, _ = roc_curve(y_test,  y_predict_nb)\n",
    "  auc_plots.append([fpr,tpr,auc])\n",
    " \n",
    "#Average and Stdv of k-fold CV\n",
    "print('Average Result of {} CV'.format(10))\n",
    "print('Accuracy    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
    "print('Recall      : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
    "print('Precision   : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
    "print('ROC-AUC     : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
    "print('F1 Score    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
    "print('===================================')\n",
    "\n",
    "#Choose auc plot with highest score\n",
    "best_auc_dnn = auc_plots[np.array(res_all[3]).argmax()]\n",
    "res_all_dnn = res_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Result of 10 CV\n",
      "Accuracy    : 0.94242Â±0.003\n",
      "Recall      : 0.82592Â±0.013\n",
      "Precision   : 0.82866Â±0.017\n",
      "ROC-AUC     : 0.89582Â±0.005\n",
      "F1 Score    : 0.82706Â±0.006\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "res_all = [[],[],[],[],[]]\n",
    "auc_plots = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42,shuffle=True)\n",
    "for train_ind, test_ind in cv.split(X_res2, y_res2):\n",
    "  #Train the model\n",
    "  X_train,y_train = X_res2.iloc[train_ind,:],y_res2[train_ind]\n",
    "  X_test,y_test = X_res2.iloc[test_ind,:],y_res2[test_ind]\n",
    "\n",
    "  #Fitting model\n",
    "  GBC = GradientBoostingClassifier(random_state=123)\n",
    "  GBC.fit(X_train,y_train)\n",
    "\n",
    "  #Predict\n",
    "  y_predict_gbc = GBC.predict(X_test)\n",
    "\n",
    "  #Calculate metrics\n",
    "  accu = accuracy_score(y_test, y_predict_gbc)\n",
    "  auc = roc_auc_score(y_test, y_predict_gbc)\n",
    "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_predict_gbc, average='binary',pos_label=1)\n",
    "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_predict_gbc, average='binary',pos_label=0)\n",
    "\n",
    "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
    "  fpr, tpr, _ = roc_curve(y_test,  y_predict_gbc)\n",
    "  auc_plots.append([fpr,tpr,auc])\n",
    " \n",
    "#Average and Stdv of k-fold CV\n",
    "print('Average Result of {} CV'.format(10))\n",
    "print('Accuracy    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
    "print('Recall      : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
    "print('Precision   : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
    "print('ROC-AUC     : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
    "print('F1 Score    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
    "print('===================================')\n",
    "\n",
    "#Choose auc plot with highest score\n",
    "best_auc_dnn = auc_plots[np.array(res_all[3]).argmax()]\n",
    "res_all_dnn = res_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Result of 10 CV\n",
      "Accuracy    : 0.97910Â±0.002\n",
      "Recall      : 0.91980Â±0.010\n",
      "Precision   : 0.95320Â±0.008\n",
      "ROC-AUC     : 0.95538Â±0.005\n",
      "F1 Score    : 0.93616Â±0.007\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "#RF Baseline\n",
    "res_all = [[],[],[],[],[]]\n",
    "auc_plots = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42,shuffle=True)\n",
    "for train_ind, test_ind in cv.split(X_res2, y_res2):\n",
    "  #Train the model\n",
    "  X_train,y_train = X_res2.iloc[train_ind,:],y_res2[train_ind]\n",
    "  X_test,y_test = X_res2.iloc[test_ind,:],y_res2[test_ind]\n",
    "\n",
    "  #Fitting model\n",
    "  rfc = RandomForestClassifier(random_state = 123)\n",
    "  rfc.fit(X_train,y_train)\n",
    "\n",
    "  #Predict\n",
    "  y_predict_rfc = rfc.predict(X_test)\n",
    "\n",
    "  #Calculate metrics\n",
    "  accu = accuracy_score(y_test, y_predict_rfc)\n",
    "  auc = roc_auc_score(y_test, y_predict_rfc)\n",
    "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_predict_rfc, average='binary',pos_label=1)\n",
    "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_predict_rfc, average='binary',pos_label=0)\n",
    "\n",
    "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
    "  fpr, tpr, _ = roc_curve(y_test,  y_predict_rfc)\n",
    "  auc_plots.append([fpr,tpr,auc])\n",
    " \n",
    "#Average and Stdv of k-fold CV\n",
    "print('Average Result of {} CV'.format(10))\n",
    "print('Accuracy    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
    "print('Recall      : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
    "print('Precision   : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
    "print('ROC-AUC     : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
    "print('F1 Score    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
    "print('===================================')\n",
    "\n",
    "#Choose auc plot with highest score\n",
    "best_auc_dnn = auc_plots[np.array(res_all[3]).argmax()]\n",
    "res_all_dnn = res_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Result of 10 CV\n",
      "Accuracy    : 0.97698Â±0.003\n",
      "Recall      : 0.92641Â±0.009\n",
      "Precision   : 0.93496Â±0.012\n",
      "ROC-AUC     : 0.95675Â±0.005\n",
      "F1 Score    : 0.93064Â±0.009\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "#XGB Baseline\n",
    "res_all = [[],[],[],[],[]]\n",
    "auc_plots = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42,shuffle=True)\n",
    "for train_ind, test_ind in cv.split(X_res2, y_res2):\n",
    "  #Train the model\n",
    "  X_train,y_train = X_res2.iloc[train_ind,:],y_res2[train_ind]\n",
    "  X_test,y_test = X_res2.iloc[test_ind,:],y_res2[test_ind]\n",
    "\n",
    "  #Fitting model\n",
    "  xgbc = xgb.XGBClassifier(random_state=0)\n",
    "  xgbc.fit(X_train,y_train)\n",
    "\n",
    "  #Predict\n",
    "  y_predict_xbgc = xgbc.predict(X_test)\n",
    "\n",
    "  #Calculate metrics\n",
    "  accu = accuracy_score(y_test, y_predict_xbgc)\n",
    "  auc = roc_auc_score(y_test, y_predict_xbgc)\n",
    "  precision_score,recall_score, f1_score,_ = precision_recall_fscore_support(y_test, y_predict_xbgc, average='binary',pos_label=1)\n",
    "  _,speci,_,_ = precision_recall_fscore_support(y_test, y_predict_xbgc, average='binary',pos_label=0)\n",
    "\n",
    "  res_all[0].append(accu);res_all[1].append(recall_score);res_all[2].append(precision_score);res_all[3].append(auc);res_all[4].append(f1_score)\n",
    "  fpr, tpr, _ = roc_curve(y_test,  y_predict_xbgc)\n",
    "  auc_plots.append([fpr,tpr,auc])\n",
    " \n",
    "#Average and Stdv of k-fold CV\n",
    "print('Average Result of {} CV'.format(10))\n",
    "print('Accuracy    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[0]), np.std(res_all[0])))\n",
    "print('Recall      : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[1]), np.std(res_all[1])))\n",
    "print('Precision   : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[2]), np.std(res_all[2])))\n",
    "print('ROC-AUC     : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[3]), np.std(res_all[3])))\n",
    "print('F1 Score    : {0:.5f}Â±{1:.3f}'.format(np.mean(res_all[4]), np.std(res_all[4])))\n",
    "print('===================================')\n",
    "\n",
    "#Choose auc plot with highest score\n",
    "best_auc_dnn = auc_plots[np.array(res_all[3]).argmax()]\n",
    "res_all_dnn = res_all\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
